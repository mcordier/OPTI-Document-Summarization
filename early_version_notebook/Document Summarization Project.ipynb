{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from pprint import pprint\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import os, os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['soc.religion.christian']\n",
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "                                  categories=categories,\n",
    "                                  shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "599\n",
      "['Mr.', 'James told me Dr.', 'Brown is not available today.', 'I will try tomorrow.']\n"
     ]
    }
   ],
   "source": [
    "print(len(twenty_train.data)) #nbr de document\n",
    "text = twenty_train.data[0] #premier document\n",
    "trainer = PunktTrainer()\n",
    "trainer.INCLUDE_ALL_COLLOCS = True\n",
    "trainer.train(text)\n",
    " \n",
    "tokenizer = PunktSentenceTokenizer(trainer.get_params())\n",
    "# Test the tokenizer on a piece of text\n",
    "sentences = \"Mr. James told me Dr. Brown is not available today. I will try tomorrow.\"\n",
    " \n",
    "print(tokenizer.tokenize(sentences))\n",
    "# ['Mr. James told me Dr.', 'Brown is not available today.', 'I will try tomorrow.']\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79, 1731)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "X = tokenizer.tokenize(text)\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2),\n",
    "                                                 token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "X_train_counts = bigram_vectorizer.fit_transform(X)\n",
    "X_train_counts.shape\n",
    "print(X_train_counts.shape) #nbr documents * nombre de mots possibles\n",
    "print(type(X_train_counts))\n",
    "#print(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t0.14002800840280097\n",
      "  (0, 3)\t0.14002800840280097\n",
      "  (0, 26)\t0.14002800840280097\n",
      "  (0, 27)\t0.14002800840280097\n",
      "  (0, 30)\t0.14002800840280097\n",
      "  (0, 31)\t0.14002800840280097\n",
      "  (0, 37)\t0.14002800840280097\n",
      "  (0, 38)\t0.14002800840280097\n",
      "  (0, 44)\t0.14002800840280097\n",
      "  (0, 45)\t0.14002800840280097\n",
      "  (0, 104)\t0.14002800840280097\n",
      "  (0, 105)\t0.14002800840280097\n",
      "  (0, 190)\t0.14002800840280097\n",
      "  (0, 196)\t0.14002800840280097\n",
      "  (0, 346)\t0.14002800840280097\n",
      "  (0, 347)\t0.14002800840280097\n",
      "  (0, 473)\t0.14002800840280097\n",
      "  (0, 474)\t0.14002800840280097\n",
      "  (0, 561)\t0.14002800840280097\n",
      "  (0, 564)\t0.14002800840280097\n",
      "  (0, 614)\t0.14002800840280097\n",
      "  (0, 626)\t0.14002800840280097\n",
      "  (0, 627)\t0.14002800840280097\n",
      "  (0, 714)\t0.14002800840280097\n",
      "  (0, 717)\t0.14002800840280097\n",
      "  :\t:\n",
      "  (77, 1182)\t0.10976425998969035\n",
      "  (77, 1191)\t0.10976425998969035\n",
      "  (77, 1192)\t0.10976425998969035\n",
      "  (77, 1366)\t0.10976425998969035\n",
      "  (77, 1367)\t0.10976425998969035\n",
      "  (77, 1372)\t0.10976425998969035\n",
      "  (77, 1373)\t0.10976425998969035\n",
      "  (77, 1375)\t0.10976425998969035\n",
      "  (77, 1381)\t0.10976425998969035\n",
      "  (77, 1405)\t0.329292779969071\n",
      "  (77, 1423)\t0.10976425998969035\n",
      "  (77, 1430)\t0.10976425998969035\n",
      "  (77, 1455)\t0.10976425998969035\n",
      "  (77, 1526)\t0.10976425998969035\n",
      "  (77, 1538)\t0.10976425998969035\n",
      "  (77, 1591)\t0.329292779969071\n",
      "  (77, 1592)\t0.10976425998969035\n",
      "  (77, 1595)\t0.10976425998969035\n",
      "  (77, 1607)\t0.10976425998969035\n",
      "  (77, 1611)\t0.10976425998969035\n",
      "  (77, 1666)\t0.10976425998969035\n",
      "  (77, 1668)\t0.10976425998969035\n",
      "  (77, 1683)\t0.10976425998969035\n",
      "  (77, 1696)\t0.10976425998969035\n",
      "  (78, 340)\t1.0\n"
     ]
    }
   ],
   "source": [
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "print(X_train_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Submodular Function to maximize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covdiv_F(S, V, cluster, lambda1=0.5):\n",
    "    if len(S)==0:\n",
    "        return(0)\n",
    "    return(cov_L(S, V) + lambda1*div_R(S, V, cluster))\n",
    "\n",
    "def cov_L(S, V, alpha=0.8):\n",
    "    res = 0\n",
    "    y= V[S]\n",
    "    for x in V:\n",
    "        res1 = cosine_similarity(x.reshape(1, -1), y).sum()\n",
    "        res2 = alpha*cosine_similarity(x.reshape(1, -1), V).sum()\n",
    "        res += min(res1, res2)\n",
    "    return(res)\n",
    "\n",
    "def div_R(S, V, cluster, alpha=0.8):\n",
    "    K = len(cluster)\n",
    "    res = 0\n",
    "    for k in range(K):\n",
    "        A = list(set(S) & set(cluster[k]))\n",
    "        S_inter_Pk = V[A]\n",
    "        res1 = 0\n",
    "        for j in S_inter_Pk:\n",
    "            res1 += cosine_similarity(j.reshape(1, -1), V).sum()\n",
    "        res += np.sqrt(res1)\n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering for div_R\n",
    "from sklearn.cluster import KMeans\n",
    "X = X_train_tf.toarray()\n",
    "# Initializing KMeans\n",
    "n_clusters = 4\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "# Fitting with inputs\n",
    "kmeans = kmeans.fit(X)\n",
    "# Predicting the clusters\n",
    "labels = kmeans.predict(X)\n",
    "# Getting the cluster centers\n",
    "C = kmeans.cluster_centers_\n",
    "cluster = {k:[] for k in range(n_clusters)}\n",
    "for i in range(len(labels)):\n",
    "    cluster[labels[i]].append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def g_1(f, G, u, c, r):\n",
    "#    return((f(G + [u])-f(G))/(c([u]))**r)\n",
    "\n",
    "def f_sub(S):\n",
    "    return(covdiv_F(S, X_train_tf, cluster))\n",
    "\n",
    "def cost(S):\n",
    "    return(len(S))\n",
    "\n",
    "B = 3\n",
    "\n",
    "V = list(range(77))\n",
    "\n",
    "def greedy_submodular(f, V, c):\n",
    "    r=1\n",
    "    G = []\n",
    "    U = list(range(V.shape[0]))\n",
    "    cost = 0\n",
    "    while len(U)!=0 and B-cost>=1:\n",
    "        L = [(f(G + [u])-f(G))/(c([u]))**r for u in U]\n",
    "        k = U[np.array(L).argmax()]\n",
    "        if cost + c([k]) <= B: # and f(G + [k])- f(G) >= 0: \n",
    "            G += [k]\n",
    "            cost += c([k])\n",
    "        U.remove(k)\n",
    "        print(G)\n",
    "    L = [f([i]) for i in V if c([i])<B]\n",
    "    v = np.array(L).argmax()\n",
    "    if f(G)>f([v]):\n",
    "        return(G)\n",
    "    else:\n",
    "        return(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46]\n",
      "[46, 58]\n",
      "[46, 58, 69]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[46, 58, 69]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_submodular(f_sub, X_train_tf, cost)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TO DO :\n",
    "# --- 1) DIVERSITY FUNCTION !!! Need a clustering generator Ok !\n",
    "\n",
    "# 2) Change the cost to the # of words in one sentence (+ enhance computation (ex : Compute all g_1(f, G, u, c, r) at the beginning))\n",
    "\n",
    "# 3) Implement the metric ROUGE-N (should not be so complicated because we already transformed the data like the paper)\n",
    "\n",
    "# 4) Test it for the documents of the dataset (sklearn, \n",
    "https://archive.ics.uci.edu/ml/datasets/Legal+Case+Reports or \n",
    "https://www-nlpir.nist.gov/related_projects/tipster_summac/cmp_lg.html). \n",
    "Verify manually if the selected sentences make sense.\n",
    "\n",
    "# 5) FURTHER : Implement the double-greedy, one local search\n",
    "\n",
    "# 6) Find a another interesting submodular function.\n",
    "\n",
    "# 7) Tune it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) New dataset : a study\n",
    "### Let's first define the metric : ROUGE-N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_n(S, X_count, gold_sum_count):\n",
    "    '''\n",
    "    S (list) : list of the index of the selected sentences S\n",
    "    \n",
    "    X_count (sparse.matrix) : sparse matrix of the studied document.  \n",
    "        X_count[k,s] : number of times the bigram s occurs in the sentence k\n",
    "        \n",
    "    gold_sum_count (list of sparse matrix)  for each sparse matrix,  \n",
    "        gold_sum_count[k][i,j] : number of times the bigram j occurs in the sentence \n",
    "        in the summary k'''\n",
    "    X_count_S_1 = X_count[S].sum(axis=0) #delete the sentence structure\n",
    "    # X_count_S_1[k,s] : number of times the bigram s occurs in the summary S\n",
    "    gold_summ_count_1 = [k.sum(axis=0) for k in gold_sum_count] #delete the sentence structure\n",
    "    num = 0\n",
    "    denom = 0\n",
    "    for i in range(len(gold_summ_count_1)):\n",
    "        for j in range(X_count_S_1.shape[1]):\n",
    "            r_ei = gold_summ_count_1[i][0,j]\n",
    "            if r_ei != 0:\n",
    "                c_es = X_count_S_1[0,j]\n",
    "                num += min(c_es, r_ei)\n",
    "                denom += r_ei\n",
    "    res = num/denom\n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Import Data (opinosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65, 1328)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "  (0, 53)\t0.30151134457776363\n",
      "  (0, 130)\t0.30151134457776363\n",
      "  (0, 142)\t0.30151134457776363\n",
      "  (0, 596)\t0.30151134457776363\n",
      "  (0, 619)\t0.30151134457776363\n",
      "  (0, 1227)\t0.6030226891555273\n",
      "  (0, 1228)\t0.30151134457776363\n",
      "  (0, 1234)\t0.30151134457776363\n",
      "  (1, 53)\t0.15617376188860607\n",
      "  (1, 59)\t0.15617376188860607\n",
      "  (1, 266)\t0.15617376188860607\n",
      "  (1, 268)\t0.15617376188860607\n",
      "  (1, 333)\t0.15617376188860607\n",
      "  (1, 339)\t0.15617376188860607\n",
      "  (1, 417)\t0.15617376188860607\n",
      "  (1, 420)\t0.15617376188860607\n",
      "  (1, 424)\t0.15617376188860607\n",
      "  (1, 431)\t0.15617376188860607\n",
      "  (1, 449)\t0.15617376188860607\n",
      "  (1, 454)\t0.15617376188860607\n",
      "  (1, 473)\t0.15617376188860607\n",
      "  (1, 582)\t0.15617376188860607\n",
      "  (1, 583)\t0.15617376188860607\n",
      "  (1, 737)\t0.15617376188860607\n",
      "  (1, 740)\t0.15617376188860607\n",
      "  :\t:\n",
      "  (63, 1128)\t0.12216944435630522\n",
      "  (63, 1133)\t0.12216944435630522\n",
      "  (63, 1147)\t0.12216944435630522\n",
      "  (63, 1255)\t0.12216944435630522\n",
      "  (63, 1256)\t0.12216944435630522\n",
      "  (64, 45)\t0.20851441405707477\n",
      "  (64, 46)\t0.20851441405707477\n",
      "  (64, 130)\t0.20851441405707477\n",
      "  (64, 154)\t0.20851441405707477\n",
      "  (64, 170)\t0.20851441405707477\n",
      "  (64, 183)\t0.20851441405707477\n",
      "  (64, 279)\t0.20851441405707477\n",
      "  (64, 280)\t0.20851441405707477\n",
      "  (64, 781)\t0.20851441405707477\n",
      "  (64, 787)\t0.41702882811414954\n",
      "  (64, 791)\t0.20851441405707477\n",
      "  (64, 797)\t0.20851441405707477\n",
      "  (64, 832)\t0.20851441405707477\n",
      "  (64, 833)\t0.20851441405707477\n",
      "  (64, 973)\t0.20851441405707477\n",
      "  (64, 974)\t0.20851441405707477\n",
      "  (64, 995)\t0.20851441405707477\n",
      "  (64, 998)\t0.20851441405707477\n",
      "  (64, 1164)\t0.20851441405707477\n",
      "  (64, 1165)\t0.20851441405707477\n"
     ]
    }
   ],
   "source": [
    "path = 'OpinosisDataset1.0_0/topics/accuracy_garmin_nuvi_255W_gps.txt.data'\n",
    "text = open(path,'r')\n",
    "text = text.read()\n",
    "\n",
    "trainer = PunktTrainer()\n",
    "trainer.INCLUDE_ALL_COLLOCS = True\n",
    "trainer.train(text)\n",
    "tokenizer = PunktSentenceTokenizer(trainer.get_params())\n",
    "\n",
    "X = tokenizer.tokenize(text)\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2),\n",
    "                                               token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "X_train_counts = bigram_vectorizer.fit_transform(X)\n",
    "X_train_counts.shape\n",
    "print(X_train_counts.shape) #nbr documents * nombre de mots possibles\n",
    "print(type(X_train_counts))\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "print(X_train_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering for div_R\n",
    "from sklearn.cluster import KMeans\n",
    "X = X_train_tf.toarray()\n",
    "# Initializing KMeans\n",
    "n_clusters = 4\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "# Fitting with inputs\n",
    "kmeans = kmeans.fit(X)\n",
    "# Predicting the clusters\n",
    "labels = kmeans.predict(X)\n",
    "# Getting the cluster centers\n",
    "C = kmeans.cluster_centers_\n",
    "cluster = {k:[] for k in range(n_clusters)}\n",
    "for i in range(len(labels)):\n",
    "    cluster[labels[i]].append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35]\n",
      "[35, 55]\n",
      "[35, 55, 49]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[35, 55, 49]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greedy_submodular(f_sub, X_train_tf, cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "['This unit is generally quite accurate.', 'Set-up and usage are considered to be very easy.', 'The maps can be updated, and tend to be reliable.']\n",
      "['The Garmin seems to be generally very accurate.', \"It's easy to use with an intuitive interface.\"]\n",
      "['It is very accurate, even in destination time.']\n",
      "['Very accurate with travel and destination time.', 'Negatives are not accurate with speed limits and rural roads.']\n",
      "['Its accurate, fast and its simple operations make this a for sure buy.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<3x1328 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 29 stored elements in Compressed Sparse Row format>,\n",
       " <2x1328 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 25 stored elements in Compressed Sparse Row format>,\n",
       " <1x1328 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 13 stored elements in Compressed Sparse Row format>,\n",
       " <2x1328 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 22 stored elements in Compressed Sparse Row format>,\n",
       " <1x1328 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 8 stored elements in Compressed Sparse Row format>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'OpinosisDataset1.0_0/summaries-gold/accuracy_garmin_nuvi_255W_gps'\n",
    "K = len([name for name in os.listdir(path)])\n",
    "print(K)\n",
    "gold_summ_txt = []\n",
    "gold_summ_count = []\n",
    "for i in range(1,K+1):\n",
    "    text2 = open(path + '/accuracy_garmin_nuvi_255W_gps.{}.gold'.format(i),'r')\n",
    "    txt = text2.read()\n",
    "    print(tokenizer.tokenize(txt))\n",
    "    gold_summ_txt.append(txt)\n",
    "    a = tokenizer.tokenize(txt)\n",
    "    if len(a)!=0:\n",
    "        gold_summ_count.append(bigram_vectorizer.transform((a)))\n",
    "gold_summ_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
